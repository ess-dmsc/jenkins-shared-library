# The ECDC Jenkins Pipeline Library

This documents describes the public interface of the ECDC Jenkins Pipeline Library and is a companion to the example Jenkinsfiles available in the *examples* folder. The instructions assume you have added the shared library globally to Jenkins using the name `ecdc-pipeline`, according to the *Making the library available in Jenkins* section of the *README.md* file.


## Making the library available in the pipeline script

Add the following line to a Jenkinsfile to make the library available:

```
@Library('ecdc-pipeline')
```

## Importing classes from the library

To make library classes available in the pipeline, use `import`:

```
import ecdcpipeline.ContainerBuildNode
import ecdcpipeline.PipelineBuilder
```


## Selecting container build nodes

The pipeline builder constructor expects a map with *ContainerBuildNode* objects as values as one of its arguments. The map keys are user-selected strings identifying each container and the values can be created in two different ways:

```
containerBuildNodes = [
  'centos': ContainerBuildNode.getDefaultContainerBuildNode('centos7'),
  'ubuntu': new ContainerBuildNode('essdmscdm/ubuntu18.04-build-node:1.1.0', 'bash -e')
]
```

### `static ContainerBuildNode getDefaultContainerBuildNode(String os)`

Return a default container build node for the operating system `os`. The valid values for this parameter are the keys in `DefaultContainerBuildNodeImages` (defined in *src/DefaultContainerBuildNodeImages.groovy*). This is the recommended approach for Conan package builds.

### `ContainerBuildNode(String image, String shell)`

The *ContainerBuildNode* constructor takes a Docker image name and the shell command to be used with it. This is the recommended approach for regular builds.


## The pipeline builder

The *PipelineBuilder* class provides the interface for creating a parallel pipeline to be run on the selected build node containers.

### `PipelineBuilder(script, containerBuildNodes)`

The *PipelineBuilder* constructor takes a reference to the current pipeline script (`this`) and a map of container build nodes as described above:

```
pipelineBuilder = new PipelineBuilder(this, containerBuildNodes)
```

A *PipelineBuilder* object has string fields that can be used in the build script:

* `project`
* `branch`
* `buildNumber`
* `baseContainerName`

```
sh "mkdir ${pipelineBuilder.project}"
echo pipelineBuilder.branch
```

### `activateEmailFailureNotifications()`

Activate email failure notifications for exceptions inside the *PipelineBuilder* `stage` method.

```
pipelineBuilder.activateEmailFailureNotifications()
```

### `activateSlackFailureNotifications()`

Activate Slack failure notifications for exceptions inside the *PipelineBuilder* `stage` method.

```
pipelineBuilder.activateSlackFailureNotifications()
```

### `createBuilders(Closure pipeline)`

Return a map of builders to be passed to a Jenkins `parallel` step. The argument is a parameterised *Closure* defined with curly braces and the parameter name before an arrow, where the parameter is the container interface (see section below):

```
builders = pipelineBuilder.createBuilders { container ->
  // Stages and steps here.
}
```

### `stage(String name, Closure stageCommands)`

Return a Jenkins pipeline stage enclosed in a `try`/`catch` block, calling the set failure notification handlers (see `activateEmailFailureNotifications` and `activateEmailFailureNotifications` above). The name can be passed inside parentheses and the *Closure* as a block between curly braces:

```
pipelineBuilder.stage("The Stage Name") {
  // Steps here.
}
```


## The container interface

Inside a `createBuilders` argument block, standard Jenkins pipeline steps will be executed on the allocated build node *outside* the container. To run commands with the container, call the methods below on the *Closure* parameter (named `container` in the examples presented in this document).

### `sh(String shellScript)`

Run a shell script inside the container:

```
container.sh "# Simple script"
container.sh """
  # Multiline script
"""
```

### `copyTo(String src, String dst)`

Copy `src` in the build node to `dst` in the container. If `dst` is a relative path, it gets prefixed with `/home/jenkins/`.

```
container.copyTo('code', "pipelineBuilder.project")
```

### `copyFrom(String src, String dst)`

Copy `src` in the container to `dst` in the build node. If `src` is a relative path, it gets prefixed with `/home/jenkins/`.

```
container.copyFrom('build', '.')
```


## Automated Conan package generation and upload

The automated Conan packaging functionality uses the *ConanPackageBuilder* class instead to create builders:

```
import ecdcpipeline.ConanPackageBuilder
```

### `ConanPackageBuilder(script, containerBuildNodes, String conanPackageChannel='stable')`

The *ConanPackageBuilder* constructor takes a reference to the current pipeline script (`this`), a map of container build nodes as described above and the Conan package channel as a string (with a default value of `stable`):

```
packageBuilder = new ConanPackageBuilder(this, containerBuildNodes, conanPackageChannel)
```

### `defineRemoteUploadNode(String containerBuildNodeKey)`

Set the container build node key to use for uploading the package recipe to the remote repository.

```
packageBuilder.defineRemoteUploadNode('centos')
```

### `skipUpload()`

Skip recipe and package upload stage.

```
packageBuilder.skipUpload()
```

### `createPackageBuilders(Closure configurations)`

Return a map of builders to be passed to a Jenkins `parallel` step, with automated local Conan server setup and upload to the local and remote servers. The argument is a parameterised *Closure* defined with curly braces and the parameter name before an arrow, where the parameter is the container interface (see section above):

```
builders = packageBuilder.createPackageBuilders { container ->
  // Configurations here.
}
```

### `def addConfiguration(Container container)`
### `def addConfiguration(Container container, settingsAndOptions)`

Add configurations to a `createPackageBuilders` argument closure. The single-argument version uses default settings and options values, while the two-argument version takes a map with optional `settings` and `options` keys, whose associated values are maps of propertyâ€“value pairs:

```
packageBuilder.addConfiguration(container, [
  'settings': [
    'librdkafka:build_type': 'Debug'
  ],
  'options': [
    'librdkafka:shared': 'False'
  ]
])
packageBuilder.addConfiguration(container, [
  'options': [
    'librdkafka:shared': 'True'
  ]
])
packageBuilder.addConfiguration(container)
```
